
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Machine Learning with Python</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/claat-public/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14" ga4id=""></google-codelab-analytics>
  <google-codelab codelab-gaid=""
                  codelab-ga4id=""
                  id="machine-learning-python"
                  title="Machine Learning with Python"
                  environment="web"
                  feedback-link="">
    
      <google-codelab-step label="Machine Learning Fundamentals" duration="10">
        <h2 is-upgraded>Labels</h2>
<p>A label is the thing we&#39;re predicting—the y variable in simple linear regression. The label could be the future price of wheat, the kind of animal shown in a picture, the meaning of an audio clip, or just about anything.</p>
<h2 is-upgraded>Features</h2>
<p>A feature is an input variable—the x variable in simple linear regression. A simple machine learning project might use a single feature, while a more sophisticated machine learning project could use millions of features.</p>
<p>Examples:</p>
<ul>
<li>words in the email text</li>
<li>sender&#39;s address</li>
<li>time of day the email was sent</li>
<li>email contains the phrase &#34;one weird trick.&#34;</li>
</ul>
<h2 is-upgraded>Examples</h2>
<p>An example is a particular instance of data, x. We break examples into two categories:</p>
<ul>
<li>labeled examples: {features, label}: (x, y)</li>
<li>unlabeled examples: {features, ?}: (x, ?)</li>
</ul>
<h2 is-upgraded>Models</h2>
<p>A model defines the relationship between features and label.</p>
<h2 is-upgraded>Regression vs. classification</h2>
<p>A regression model predicts continuous values, while a classification model predicts discrete values.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Scalar, Vector, Matrix, &amp; Tensor" duration="10">
        <p class="image-container"><img src="/scalar.png"></p>
<h2 is-upgraded>Scalar</h2>
<p>A scalar is simply a single number. For example 24.</p>
<h2 is-upgraded>Vector</h2>
<p>A vector is an ordered array of numbers and can be in a row or a column. A vector has just a single index, which can point to a specific value within the vector. For example, V2 refers to the second value within the vector, which is -8 in the graphic above.</p>
<h2 is-upgraded>Matrix</h2>
<p>A matrix is an ordered 2D array of numbers and it has two indices. The first one points to the row and the second one to the column. For example, M23 refers to the value in the second row and the third column, which is 8 in the yellow graphic above. A matrix can have multiple numbers of rows and columns. Note that a vector is also a matrix, but with only one row or one column.</p>
<p class="image-container"><img src="/tensor_0.png"></p>
<h2 is-upgraded>Tensor</h2>
<p>You can think of a tensor as an array of numbers, arranged on a regular grid, with a variable number of axes. A tensor has three indices, where the first one points to the row, the second to the column and the third one to the axis. For example, T232 points to the second row, the third column, and the second axis. This refers to the value 0 in the right tensor in the graphic below:</p>


      </google-codelab-step>
    
      <google-codelab-step label="Linear Regression" duration="20">
        <h2 is-upgraded>Introduction</h2>
<p>Linear regression is a fundamental statistical technique used for modeling the relationship between a dependent variable (target) and one or more independent variables (features). In this code lab, we&#39;ll implement simple linear regression to predict a continuous variable based on a single input feature.</p>
<h2 is-upgraded>Step 1: Import Libraries</h2>
<p>We start by importing the necessary libraries:</p>
<pre><code language="language-python" class="language-python">import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
</code></pre>
<p><code>numpy: For numerical operations and array manipulations.<br> train_test_split: For splitting the data into training and test sets.<br> LinearRegression: From scikit-learn, for implementing linear regression.<br> mean_squared_error: For evaluating the performance of the model.<br> matplotlib.pyplot: For data visualization.<br></code></p>
<h2 is-upgraded>Step 2: Generate Sample Data</h2>
<p>We generate sample data with a linear relationship and some random noise:</p>
<pre><code language="language-python" class="language-python">np.random.seed(0)
X = 2 * np.random.rand(100, 1)  # Generate 100 random numbers between 0 and 2
y = 4 + 3 * X + np.random.randn(100, 1)  # Linear relationship with noise
</code></pre>
<p><code>X: Input feature (independent variable).<br> y: Target variable (dependent variable).<br></code></p>
<h2 is-upgraded>Step 3: Split Data</h2>
<p>We split the data into training and test sets:</p>
<pre><code language="language-python" class="language-python">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
</code></pre>
<h2 is-upgraded>Step 4: Train the Model</h2>
<p>We train a linear regression model on the training data:</p>
<pre><code language="language-python" class="language-python">model = LinearRegression()
model.fit(X_train, y_train)
</code></pre>
<h2 is-upgraded>Step 5: Make Predictions</h2>
<p>We use the trained model to make predictions on the test data:</p>
<pre><code language="language-python" class="language-python">y_pred = model.predict(X_test)
</code></pre>
<h2 is-upgraded>Step 6: Evaluate the Model</h2>
<p>We calculate the mean squared error to evaluate the performance of the model:</p>
<pre><code language="language-python" class="language-python">mse = mean_squared_error(y_test, y_pred)
</code></pre>
<h2 is-upgraded>Step 7: Visualization</h2>
<p>We plot the test data points and the regression line:</p>
<pre><code language="language-python" class="language-python">plt.scatter(X_test, y_test, color=&#39;blue&#39;)
plt.plot(X_test, y_pred, color=&#39;red&#39;)
plt.title(&#39;Linear Regression&#39;)
plt.xlabel(&#39;X&#39;)
plt.ylabel(&#39;y&#39;)
plt.show()
</code></pre>
<h2 is-upgraded>Step 8: Output</h2>
<p>We print the coefficients of the linear regression model, the intercept, and the mean squared error:</p>
<pre><code language="language-python" class="language-python">print(&#39;Coefficients:&#39;, model.coef_)
print(&#39;Intercept:&#39;, model.intercept_)
print(&#39;Mean Squared Error:&#39;, mse)
</code></pre>
<h2 is-upgraded>Conclusion</h2>
<p>In this code lab, we implemented simple linear regression to predict a continuous variable based on a single input feature. We trained the model, made predictions, evaluated its performance, and visualized the results. Linear regression is a powerful technique used in various fields, including finance, economics, and machine learning. Experiment with different datasets and explore more advanced regression techniques for more complex modeling tasks.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Logistic Regression" duration="20">
        <h2 is-upgraded>Introduction</h2>
<p>Logistic regression is a popular classification algorithm used for binary classification tasks. It models the probability that a given input belongs to a particular class. In this code lab, we&#39;ll implement logistic regression using the sigmoid function and apply it to a binary classification problem.</p>
<h2 is-upgraded>Step 1: Import Libraries</h2>
<p>We start by importing the necessary libraries:</p>
<pre><code language="language-python" class="language-python">import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
</code></pre>
<p><code>numpy: For numerical operations and array manipulations.<br> train_test_split: For splitting the data into training and test sets.<br> LogisticRegression: From scikit-learn, for implementing logistic regression.<br> accuracy_score: For evaluating the accuracy of the model.<br> confusion_matrix: For evaluating the performance of the model.<br> matplotlib.pyplot: For data visualization.<br></code></p>
<h2 is-upgraded>Step 2: Generate Sample Data</h2>
<p>We generate sample data for a binary classification task:</p>
<pre><code language="language-python" class="language-python">np.random.seed(0)
X = np.random.rand(100, 2)  # Generate 100 samples with 2 features
y = (X[:, 0] + X[:, 1] &gt; 1).astype(int)  # Classify based on the sum of features
</code></pre>
<p><code>X: Input features.<br> y: Target variable (binary labels).<br></code></p>
<h2 is-upgraded>Step 3: Split Data</h2>
<p>We split the data into training and test sets:</p>
<pre><code language="language-python" class="language-python">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
</code></pre>
<h2 is-upgraded>Step 4: Train the Model</h2>
<p>We train a logistic regression model on the training data:</p>
<pre><code language="language-python" class="language-python">model = LogisticRegression()
model.fit(X_train, y_train)
</code></pre>
<h2 is-upgraded>Step 5: Make Predictions</h2>
<p>We use the trained model to make predictions on the test data:</p>
<pre><code language="language-python" class="language-python">y_pred = model.predict(X_test)
</code></pre>
<h2 is-upgraded>Step 6: Evaluate the Model</h2>
<p>We evaluate the performance of the model using accuracy score and confusion matrix:</p>
<pre><code language="language-python" class="language-python">accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
</code></pre>
<h2 is-upgraded>Step 7: Visualization</h2>
<p>We visualize the decision boundary of the logistic regression model:</p>
<pre><code language="language-python" class="language-python">plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.coolwarm)
plt.xlabel(&#39;Feature 1&#39;)
plt.ylabel(&#39;Feature 2&#39;)
plt.title(&#39;Logistic Regression Decision Boundary&#39;)
plt.show()
</code></pre>
<h2 is-upgraded>Step 8: Output</h2>
<p>We print the accuracy score and confusion matrix:</p>
<pre><code language="language-python" class="language-python">print(&#39;Accuracy:&#39;, accuracy)
print(&#39;Confusion Matrix:&#39;)
print(conf_matrix)
</code></pre>
<h2 is-upgraded>Conclusion</h2>
<p>In this code lab, we implemented logistic regression for a binary classification task. We generated sample data, trained the model, made predictions, and evaluated its performance. Logistic regression is a versatile algorithm used in various applications, including spam detection, medical diagnosis, and credit scoring. Experiment with different datasets and explore advanced techniques to improve classification accuracy.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Cost Functions" duration="20">
        <h2 is-upgraded>Introduction</h2>
<p>Cost functions play a crucial role in training machine learning models by quantifying the error between predicted values and actual values. In this code lab, we&#39;ll focus on Mean Squared Error (MSE), a commonly used cost function for regression problems. We&#39;ll implement MSE from scratch and use it to evaluate the performance of a regression model.</p>
<h2 is-upgraded>Step 1: Implement Mean Squared Error (MSE)</h2>
<p>We start by implementing the Mean Squared Error (MSE) function:</p>
<pre><code language="language-python" class="language-python">import numpy as np

def mean_squared_error(y_true, y_pred):
    &#34;&#34;&#34;
    Calculate the Mean Squared Error (MSE) between true and predicted values.

    Parameters:
    y_true : array-like, true values
    y_pred : array-like, predicted values

    Returns:
    mse : float, mean squared error
    &#34;&#34;&#34;
    N = len(y_true)
    mse = np.sum((y_true - y_pred) ** 2) / N
    return mse
</code></pre>
<h2 is-upgraded>Step 2: Generate Sample Data</h2>
<p>Next, we generate sample data for a regression problem:</p>
<pre><code language="language-python" class="language-python">np.random.seed(0)
X = 2 * np.random.rand(100, 1)  # Generate 100 random numbers between 0 and 2
y = 4 + 3 * X + np.random.randn(100, 1)  # Linear relationship with noise
</code></pre>
<h2 is-upgraded>Step 3: Split Data</h2>
<p>We split the data into training and test sets:</p>
<pre><code language="language-python" class="language-python">from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

</code></pre>
<h2 is-upgraded>Step 4: Train a Regression Model</h2>
<p>We train a regression model (e.g., linear regression) on the training data:</p>
<pre><code language="language-python" class="language-python">from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)
</code></pre>
<h2 is-upgraded>Step 5: Make Predictions</h2>
<p>We use the trained model to make predictions on the test data:</p>
<pre><code language="language-python" class="language-python">y_pred = model.predict(X_test)
</code></pre>
<h2 is-upgraded>Step 6: Evaluate the Model Using MSE</h2>
<p>We calculate the Mean Squared Error (MSE) to evaluate the performance of the regression model:</p>
<pre><code language="language-python" class="language-python">mse = mean_squared_error(y_test, y_pred)
</code></pre>
<h2 is-upgraded>Step 7: Output</h2>
<p>Finally, we print the calculated MSE:</p>
<pre><code language="language-python" class="language-python">print(&#39;Mean Squared Error (MSE):&#39;, mse)
</code></pre>
<h2 is-upgraded>Conclusion</h2>
<p>In this code lab, we implemented the Mean Squared Error (MSE) cost function from scratch and used it to evaluate the performance of a regression model. MSE is a useful metric for quantifying the discrepancy between predicted and actual values in regression problems. Experiment with different regression models and datasets to gain a deeper understanding of MSE and its implications for model performance.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Gradient Descent" duration="20">
        <h2 is-upgraded>Introduction</h2>
<p>Gradient descent is a fundamental optimization algorithm used to minimize the cost function by iteratively updating the model parameters (coefficients) in the direction of the steepest descent of the cost function gradient. In this code lab, we&#39;ll implement gradient descent to minimize a cost function and find the optimal values of the coefficients.</p>
<h2 is-upgraded>Step 1: Implement Gradient Descent</h2>
<p>We start by implementing the gradient descent algorithm:</p>
<pre><code language="language-python" class="language-python">import numpy as np

def gradient_descent(X, y, theta, alpha, num_iterations):
    &#34;&#34;&#34;
    Perform gradient descent to minimize the cost function.

    Parameters:
    X : array-like, feature matrix
    y : array-like, target variable
    theta : array-like, initial coefficients
    alpha : float, learning rate
    num_iterations : int, number of iterations

    Returns:
    theta : array-like, optimized coefficients
    cost_history : list, cost function history
    &#34;&#34;&#34;
    m = len(y)
    cost_history = []

    for i in range(num_iterations):
        # Calculate predictions
        predictions = np.dot(X, theta)

        # Calculate error
        error = predictions - y

        # Calculate gradient
        gradient = np.dot(X.T, error) / m

        # Update coefficients
        theta -= alpha * gradient

        # Calculate cost
        cost = np.sum(error ** 2) / (2 * m)
        cost_history.append(cost)

    return theta, cost_history
</code></pre>
<h2 is-upgraded>Step 2: Generate Sample Data</h2>
<p>Next, we generate sample data for a regression problem:</p>
<pre><code language="language-python" class="language-python">np.random.seed(1)
X = 2 * np.random.rand(100, 1)  # Generate 100 random numbers between 0 and 2
y = 4 + 3 * X + np.random.randn(100, 1)  # Linear relationship with noise
</code></pre>
<h2 is-upgraded>Step 3: Add Bias Term and Normalize Features</h2>
<p>We add a bias term (intercept) and normalize the features:</p>
<pre><code language="language-python" class="language-python">X_b = np.c_[np.ones((100, 1)), X]  # Add bias term
X_normalized = (X_b - np.mean(X_b, axis=0)) / np.std(X_b[:, 1:], axis=0)  # Normalize features, excluding intercept
</code></pre>
<h2 is-upgraded>Step 4: Initialize Coefficients and Hyperparameters</h2>
<p>We initialize the coefficients and hyperparameters:</p>
<pre><code language="language-python" class="language-python">theta = np.random.randn(2, 1)  # Initialize coefficients
alpha = 0.01  # Learning rate
num_iterations = 10000  # Number of iterations
</code></pre>
<h2 is-upgraded>Step 5: Perform Gradient Descent</h2>
<p>We perform gradient descent to minimize the cost function:</p>
<pre><code language="language-python" class="language-python">theta_optimized, cost_history = gradient_descent(X_normalized, y, theta, alpha, num_iterations)
</code></pre>
<h2 is-upgraded>Step 6: Output</h2>
<p>Finally, we print the optimized coefficients and visualize the cost function history:</p>
<pre><code language="language-python" class="language-python">print(&#39;Optimized Coefficients:&#39;, theta_optimized)

import matplotlib.pyplot as plt

plt.plot(range(num_iterations), cost_history)
plt.xlabel(&#39;Iterations&#39;)
plt.ylabel(&#39;Cost&#39;)
plt.title(&#39;Cost Function History&#39;)
plt.show()
</code></pre>
<h2 is-upgraded>Conclusion</h2>
<p>In this code lab, we implemented the gradient descent algorithm to minimize a cost function and find the optimal values of the coefficients. Gradient descent is a powerful optimization technique widely used in machine learning for training models. Experiment with different learning rates and numbers of iterations to observe their effects on convergence and optimization performance.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Matrix Operations" duration="20">
        <h2 is-upgraded>Introduction</h2>
<p>Matrix operations are fundamental in many machine learning algorithms, enabling efficient computation and manipulation of data. In this code lab, we&#39;ll focus on some common matrix operations, including matrix multiplication, transpose, and inverse.</p>
<h2 is-upgraded>Step 1: Perform Matrix Multiplication</h2>
<p>We start by implementing matrix multiplication:</p>
<pre><code language="language-python" class="language-python">import numpy as np

def matrix_multiplication(A, B):
    &#34;&#34;&#34;
    Perform matrix multiplication.

    Parameters:
    A : array-like, first matrix
    B : array-like, second matrix

    Returns:
    C : array-like, result of matrix multiplication (C = A * B)
    &#34;&#34;&#34;
    return np.dot(A, B)
</code></pre>
<h2 is-upgraded>Step 2: Generate Sample Data</h2>
<p>Next, we generate sample data for matrices A and B:</p>
<pre><code language="language-python" class="language-python">A = np.array([[1, 2], [3, 4]])  # Sample matrix A
B = np.array([[5, 6], [7, 8]])  # Sample matrix B
</code></pre>
<h2 is-upgraded>Step 3: Perform Matrix Multiplication</h2>
<p>We perform matrix multiplication using the implemented function:</p>
<pre><code language="language-python" class="language-python">C = matrix_multiplication(A, B)
</code></pre>
<h2 is-upgraded>Step 4: Output</h2>
<p>Finally, we print the result of matrix multiplication:</p>
<pre><code language="language-python" class="language-python">print(&#39;Result of Matrix Multiplication (C = A * B):&#39;)
print(C)
</code></pre>
<h2 is-upgraded>Conclusion</h2>
<p>In this code lab, we implemented matrix multiplication, a fundamental operation in linear algebra and many machine learning algorithms. Matrix operations enable efficient computation and manipulation of data, facilitating the development and optimization of machine learning models. Experiment with different matrices and observe their effects on the result of matrix multiplication.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Naive Bayes" duration="20">
        <h2 is-upgraded>Introduction</h2>
<p>Naive Bayes is a simple yet powerful classification algorithm based on Bayes&#39; theorem with the assumption of independence between features. It&#39;s widely used for text classification, spam filtering, and sentiment analysis. In this code lab, we&#39;ll implement the Gaussian Naive Bayes algorithm for a binary classification task.</p>
<h2 is-upgraded>Step 1: Import Libraries</h2>
<p>We start by importing the necessary libraries:</p>
<pre><code language="language-python" class="language-python">import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
</code></pre>
<p><code>numpy: For numerical operations and array manipulations.<br> train_test_split: For splitting the data into training and test sets.<br> make_classification: For generating synthetic classification datasets.<br> GaussianNB: From scikit-learn, for implementing Gaussian Naive Bayes.<br> accuracy_score, classification_report, confusion_matrix: For evaluating the performance of the model.<br></code></p>
<h2 is-upgraded>Step 2: Generate Sample Data</h2>
<p>Next, we generate sample data for a binary classification task:</p>
<pre><code language="language-python" class="language-python">X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
</code></pre>
<h2 is-upgraded>Step 3: Split Data</h2>
<p>We split the data into training and test sets:</p>
<pre><code language="language-python" class="language-python">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
</code></pre>
<h2 is-upgraded>Step 4: Train the Model</h2>
<p>We train a Gaussian Naive Bayes model on the training data:</p>
<pre><code language="language-python" class="language-python">model = GaussianNB()
model.fit(X_train, y_train)
</code></pre>
<h2 is-upgraded>Step 5: Make Predictions</h2>
<p>We use the trained model to make predictions on the test data:</p>
<pre><code language="language-python" class="language-python">y_pred = model.predict(X_test)
</code></pre>
<h2 is-upgraded>Step 6: Evaluate the Model</h2>
<p>We evaluate the performance of the model using accuracy score, classification report, and confusion matrix:</p>
<pre><code language="language-python" class="language-python">accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)
matrix = confusion_matrix(y_test, y_pred)
</code></pre>
<h2 is-upgraded>Step 7: Output</h2>
<p>Finally, we print the evaluation metrics:</p>
<pre><code language="language-python" class="language-python">print(&#39;Accuracy:&#39;, accuracy)
print(&#39;Classification Report:\n&#39;, report)
print(&#39;Confusion Matrix:\n&#39;, matrix)
</code></pre>
<h2 is-upgraded>Conclusion</h2>
<p>In this code lab, we implemented the Gaussian Naive Bayes algorithm for a binary classification task. Naive Bayes is a simple yet effective algorithm, especially for text classification and other tasks with high-dimensional feature spaces. Experiment with different datasets and explore the capabilities and limitations of Naive Bayes for classification.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Sigmoid Function" duration="20">
        <h2 is-upgraded>Introduction</h2>
<p>The sigmoid function, also known as the logistic function, is a commonly used activation function in binary classification tasks and logistic regression. It maps any real-valued number to the range [0, 1], making it suitable for modeling probabilities. In this code lab, we&#39;ll implement the sigmoid function and explore its properties.</p>
<h2 is-upgraded>Step 1: Implement the Sigmoid Function</h2>
<p>We start by implementing the sigmoid function:</p>
<pre><code language="language-python" class="language-python">import numpy as np

def sigmoid(z):
    &#34;&#34;&#34;
    Compute the sigmoid function.

    Parameters:
    z : array-like, input value or array

    Returns:
    sigmoid_value : array-like, output of the sigmoid function
    &#34;&#34;&#34;
    return 1 / (1 + np.exp(-z))
</code></pre>
<h2 is-upgraded>Step 2: Generate Sample Data</h2>
<p>Next, let&#39;s generate some sample data to visualize the sigmoid function:</p>
<pre><code language="language-python" class="language-python">import matplotlib.pyplot as plt

# Generate values for the input range
z = np.linspace(-10, 10, 100)
# Compute sigmoid values
sigmoid_values = sigmoid(z)
</code></pre>
<h2 is-upgraded>Step 3: Output</h2>
<p>Finally, we visualize the sigmoid function by plotting its curve:</p>
<pre><code language="language-python" class="language-python"># Plot the sigmoid function
plt.plot(z, sigmoid_values)
plt.xlabel(&#39;z&#39;)
plt.ylabel(&#39;Sigmoid(z)&#39;)
plt.title(&#39;Sigmoid Function&#39;)
plt.grid(True)
plt.show()
</code></pre>
<h2 is-upgraded>Conclusion</h2>
<p>In this code lab, we implemented the sigmoid function, a key component in logistic regression and neural networks. The sigmoid function maps any real-valued input to the range [0, 1], making it suitable for binary classification tasks where we want to model probabilities. Experiment with different input ranges and explore the behavior of the sigmoid function.</p>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/claat-public/native-shim.js"></script>
  <script src="https://storage.googleapis.com/claat-public/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/claat-public/prettify.js"></script>
  <script src="https://storage.googleapis.com/claat-public/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
